"""
 * @author 党智腾
 * mail 642212607@qq.com
 * WeChat dangzhiteng
 * @date 2018-06-01
 * @version V 2.0
"""

import re
import json
import time
import random
import requests
from threading import Thread
from multiprocessing import Process, Queue, cpu_count
from bs4 import BeautifulSoup


start = time.time()
headers = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
           'Accept-Encoding': 'gzip, deflate, br',
           'Accept-Language': 'zh-CN,zh;q=0.9',
           'Cache-Control': 'max-age=0',
           'Connection': 'keep-alive',
           'Host': 'movie.douban.com',
           'Referer': 'https://movie.douban.com/',
           'Upgrade-Insecure-Requests': '1',
           'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
                         ' AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36'}
processnum = cpu_count()  # CPU数目，决定开启几个进程
number = 0  # 计数
databuffer = Queue(100)
#    url = "https://movie.douban.com/subject/{0}/".format(id)


# 更新IP池
def refreship():
    proxiespool = []
    myip = '220.202.152.95'
    testurl = 'http://2018.ip138.com/ic.asp'
    # url = 'http://www.xicidaili.com/'  # 西刺代理IP 高匿
    url = 'http://www.xicidaili.com/wt/'  # 西刺代理IP HTTP
    header = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                      'AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/64.0.3282.186 Safari/537.36'}
    html = requests.get(url, headers=header)
    pattern = re.compile('<td>\d+.\d+.\d+.\d+</td>\n      <td>\d+</td>\n      <td>\n        '
                         '.*?\n      </td>\n      .*?\n      <td>\w+</td>')
    result = re.findall(pattern, html.text)
    f = open('IP池.txt', 'w+')
    for i in range(len(result)):
        item = result[i]
        type = re.search('<td>\w+</td>$', item).group()[4:-5].lower()
        if type == 'https':
            continue
        ip = re.search('\d+.\d+.\d+.\d+', item).group()
        port = re.search('<td>\d+</td>', item).group()[4:-5]
        proxies = {type: type + '://' + ip + ':' + port}
        try:
            testhtml = requests.get(testurl, proxies=proxies)
        except:
            print(ip + ':' + port, '代理失败')
            continue
        current = testhtml.text
        currentip = re.search('\d+.\d+.\d+.\d+', current).group()
        if currentip != myip:
            print(str(i + 1) + '.', ip + ':' + port, '代理成功')
            proxiespool.append(proxies)
            f.write(json.dumps(proxies))
            f.write('\n')
        else:
            print(str(i + 1) + '.', ip + ':' + port, '代理失败')
    f.close()
    print('代理池更新完毕,新代理池大小为:'+str(len(proxiespool)))
    return proxiespool


# 爬取电影基本信息 创建评论ID队列
def douban_spider(filmid, strategy):
    proxiespool = []  # 读取IP代理池
    with open('IP池.txt', 'r', encoding='utf-8') as proxiesfile:
        for line in proxiesfile:
            proxiespool.append(json.loads(line))
    print('该电脑为', str(processnum), '核,即将开启' + str(processnum) + '进程...')
    print('代理IP池：', proxiespool)
    url = "https://movie.douban.com/subject/{0}/{1}".format(filmid, strategy)
    if strategy == 'reviews':
        # 写入头信息
        starthtml = requests.get(url, headers=headers)
        starthtml = starthtml.content.decode()
        soup = BeautifulSoup(starthtml, 'html.parser')
        count = re.search('\d+', soup.title.string.strip()).group()
        num = int(count) // 20 + 1
        del soup, starthtml, count
        # 结束写入
        idqueue = Queue(processnum * 20)  # 评论ID队列
        # 启动填充ID线程
        threadid = Thread(target=getreview, args=(num, idqueue, proxiespool))
        threadid.start()
        # 启动监控数据缓冲区线程
        threadwrite = Thread(target=writefile)
        threadwrite.start()
        processtitle = ('一号进程', '二号进程', '三号进程', '四号进程', '五号进程', '六号进程', '七号进程', '八号进程')
        # 启动爬虫进程
        for i in range(processnum):
            spider = ReviewsSpider(processtitle[i], idqueue, proxiespool)
            spider.daemon = True
            spider.start()
    else:
        # 启动填充段评论url线程
        urlqueue = Queue(processnum * 20)  # 评论ID队列
        threadurl = Thread(target=getcomment, args=(url, urlqueue))
        threadurl.start()
        # 启动监控数据缓冲区线程
        threadwrite = Thread(target=writefile)
        threadwrite.start()
        processtitle = ('一号进程', '二号进程', '三号进程', '四号进程', '五号进程', '六号进程', '七号进程', '八号进程')
        # 启动爬虫进程
        for i in range(processnum):
            spider = CommentsSpider(processtitle[i], urlqueue, proxiespool)
            spider.daemon = True
            spider.start()


# 未登录只可爬取200条短评 登录可爬取500条
# 填充段评论url线程
def getcomment(sourceurl, urlqueue):
    for i in range(10):
        currenturl = sourceurl + '?start={0}&limit=20&sort=new_score&status=P&percent_type='.format(20 * i)
        urlqueue.put(currenturl)


# 填充长评论ID队列线程
def getreview(num, idqueue, proxiespool):
    for i in range(num):
        currenturl = "https://movie.douban.com/subject/26945085/reviews" + '?start={}'.format(20 * i)
        while True:
            try:
                proxies = proxiespool[random.randint(0, len(proxiespool) - 1)]
                html = requests.get(currenturl, proxies=proxies, timeout=3)
                break
            except ConnectionError:
                print(proxies, '失效')
                pass
        soup = BeautifulSoup(html.content, 'html.parser')
        idinfo = soup.find_all(class_="review-item")  # 提取评论ID
        for item in idinfo:
            reviewid = item.find(class_="review-short")['data-rid']
            idqueue.put(reviewid)


# 获取长评论并将长评论写入数据缓冲队列进程
class ReviewsSpider(Process):
    reviewid = 0

    def __init__(self, name, queue, proxiespool):
        Process.__init__(self)
        # super(Spider, self).__init__()
        self.name = name
        self.queue = queue
        self.proxiespool = proxiespool
        self.databuffer = databuffer

    def run(self):
        self.getreview()
        print(self.name, '已终止...')

    def getreview(self):
        time.sleep(2)  # 3秒钟后开始获取评论
        while not self.queue.empty():
            try:
                self.reviewid = self.queue.get(timeout=5)
            except:
                break
            proxies = self.proxiespool[random.randint(0, len(self.proxiespool) - 1)]
            reviewurl = 'https://movie.douban.com/j/review/{0}/full'.format(self.reviewid)
            reviewhtml = requests.get(reviewurl, proxies=proxies)
            js = json.loads(reviewhtml.text)  # js = data.content.decode()
            html = js['body']
            soup = BeautifulSoup(html, 'html.parser').find(class_="review-content clearfix")
            title = soup["data-author"]
            reviewlist = soup.find_all('p')
            review = ''
            for item in reviewlist:
                currentreview = item.string
                if currentreview is not None:
                    review += currentreview
            self.databuffer.put(title + '\n' + review)
            print(self.name, '爬取完成影评', self.reviewid, '。程序已运行', str(time.time() - start))


# 获取短评论并将短评论写入数据缓冲队列进程
class CommentsSpider(Process):
    def __init__(self, name, queue, proxiespool):
        Process.__init__(self)
        # super(Spider, self).__init__()
        self.name = name
        self.queue = queue
        self.proxiespool = proxiespool
        self.databuffer = databuffer

    def run(self):
        self.getreview()
        print(self.name, '已终止...')

    def getreview(self):
        time.sleep(2)
        while not self.queue.empty():
            try:
                url = self.queue.get(timeout=5)
            except:
                break
            proxies = self.proxiespool[random.randint(0, len(self.proxiespool) - 1)]
            commentshtml = requests.get(url, proxies=proxies)
            soup = BeautifulSoup(commentshtml.text, 'html.parser')
            commentlist = soup.find_all(class_='comment-item')
            for item in commentlist:
                commentid = item["data-cid"].strip()
                username = item.a['title'].strip()
                votes = item.find(class_='votes').string.strip()
                comtime = item.find(class_="comment-time ")["title"].strip()
                comment = item.find('p').text.strip()
                self.databuffer.put('用户名:' + username + ' '*5 + '评论ID:' + commentid + ' '*5 + '投票数目:' +
                                    votes + ' '*5 + '评论时间:' + comtime + '\n' + comment)
                print(self.name, '爬取完成影评', commentid, '。程序已运行', str(time.time() - start))


# 监控数据缓冲区线程
def writefile():
    with open('影评.txt', 'w+', encoding='utf8') as f:
        while True:
            try:
                review = databuffer.get(timeout=10)
                f.write(review + '\n\n')
            except:
                print('评论队列已空')
                break
    print('评论写入完毕')


# 启动爬虫
if __name__ == '__main__':
    print('     ****************豆瓣影评爬取小程序V1.0****************     \n')
    fun = input('请选择(第一次请刷新IP池) 1）更新IP池 2）开启爬虫\n')
    while True:
        if fun in ['1', '2']:
            break
        else:
            print('请重新输入 1）更新IP池 2）开启爬虫')
            fun = input()
    if fun == '1':
        refreship()
    else:
        filmid = input("请输入电影ID:\n")
        strategy = input("请输入爬取策略(reviews/comments):\n")
        while True:
            if strategy in ['reviews', 'comments']:
                break
            else:
                print("请正确输入爬取策略(reviews/comments):\n")
                strategy = input()
        douban_spider(filmid, strategy)
