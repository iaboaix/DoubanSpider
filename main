"""
 * @author 党智腾
 * mail 642212607@qq.com
 * WeChat dangzhiteng
 * @date 2018-05-30
 * @version V 1.0
"""

import re
import json
import time
import random
import requests
from threading import Thread
from multiprocessing import Process, Queue, cpu_count
from bs4 import BeautifulSoup


start = time.time()
headers = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
           'Accept-Encoding': 'gzip, deflate, br',
           'Accept-Language': 'zh-CN,zh;q=0.9',
           'Cache-Control': 'max-age=0',
           'Connection': 'keep-alive',
           'Host': 'movie.douban.com',
           'Referer': 'https://movie.douban.com/',
           'Upgrade-Insecure-Requests': '1',
           'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
                         ' AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36'}
sourceurl = "https://movie.douban.com/subject/26945085/reviews"
processnum = cpu_count()  # CPU数目，决定开启几个进程
number = 0  # 计数
reviewpool = []  # 评论池
databuffer = Queue(100)

# 更新IP池
def refreship():
    proxiespool = []
    myip = '220.202.152.95'
    testurl = 'http://2018.ip138.com/ic.asp'
    # url = 'http://www.xicidaili.com/'  # 西刺代理IP 高匿
    url = 'http://www.xicidaili.com/wt/'  # 西刺代理IP HTTP
    header = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                      'AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/64.0.3282.186 Safari/537.36'}
    html = requests.get(url, headers=header)
    pattern = re.compile('<td>\d+.\d+.\d+.\d+</td>\n      <td>\d+</td>\n      <td>\n        '
                         '.*?\n      </td>\n      .*?\n      <td>\w+</td>')
    result = re.findall(pattern, html.text)
    f = open('IP池.txt', 'w+')
    for i in range(len(result)):
        item = result[i]
        type = re.search('<td>\w+</td>$', item).group()[4:-5].lower()
        if type == 'https':
            continue
        ip = re.search('\d+.\d+.\d+.\d+', item).group()
        port = re.search('<td>\d+</td>', item).group()[4:-5]
        proxies = {type: type + '://' + ip + ':' + port}
        try:
            testhtml = requests.get(testurl, proxies=proxies)
        except:
            print(ip + ':' + port, '代理失败')
            continue
        current = testhtml.text
        currentip = re.search('\d+.\d+.\d+.\d+', current).group()
        if currentip != myip:
            print(str(i + 1)+'.', ip + ':' + port, '代理成功')
            proxiespool.append(proxies)
            f.write(json.dumps(proxies))
        else:
            print(i+'.', ip + ':' + port, '代理失败')
    f.close()
    print('代理池更新完毕,新代理池大小为:'+str(len(proxiespool)))
    return proxiespool


# 爬取电影基本信息 创建评论ID队列
def douban_spider(url=sourceurl):
    proxiespool = []  # 读取IP代理池
    with open('IP池.txt', 'r', encoding='utf-8') as proxiesfile:
        for line in proxiesfile:
            proxiespool.append(json.loads(line))
    print('该电脑为', str(processnum), '核,即将开启' + str(processnum) + '进程...')
    print('代理IP池：', proxiespool)
    starthtml = requests.get(url, headers=headers)
    starthtml = starthtml.content.decode()
    soup = BeautifulSoup(starthtml, 'html.parser')
    f = open('影评.txt', 'w', encoding='utf-8')
    star = soup.find(class_="droplist").find_all('a')
    f.write(soup.title.string.strip() + '\n')
    for item in star:
        f.write(item.string.strip() + ' '*5)
    f.write('\n\n')
    f.close()
    num = re.search('\d+', soup.title.string.strip()).group()
    num = int(num) // 20 + 1
    idqueue = Queue(processnum * 20)  # 评论ID队列
    threadid = Thread(target=getreview, args=(num, idqueue, proxiespool))
    threadid.start()
    threadwrite = Thread(target=writefile)
    threadwrite.start()
    processtitle = ('一号进程', '二号进程', '三号进程', '四号进程', '五号进程', '六号进程', '七号进程', '八号进程')
    for i in range(processnum):
        spider = Spider(processtitle[i], idqueue, proxiespool)
        spider.daemon = True
        spider.start()


# 填充评论ID队列线程
def getreview(num, idqueue, proxiespool):
    for i in range(num):
        currenturl = sourceurl + '?start={}'.format(20 * i)
        while True:
            try:
                proxies = proxiespool[random.randint(0, len(proxiespool) - 1)]
                html = requests.get(currenturl, proxies=proxies, timeout = 20)
                break
            except ConnectionError:
                print(proxies, '失效')
                pass
        soup = BeautifulSoup(html.content, 'html.parser')
        idinfo = soup.find_all(class_="main-bd")  # 提取评论ID
        for j in range(len(idinfo)):
            reviewid = idinfo[j].find(class_="review-short")['data-rid']
            idqueue.put(reviewid)
        if i == 0:
            break


# 获取评论并将评论写入数据缓冲队列进程
class Spider(Process):
    reviewid = 0

    def __init__(self, name, queue, proxiespool):
        Process.__init__(self)
        # super(Spider, self).__init__()
        self.name = name
        self.queue = queue
        self.proxiespool = proxiespool
        self.databuffer = databuffer

    def run(self):
        self.getreview()
        print(self.name, '已终止...')

    def getreview(self):
        time.sleep(2)  # 3秒钟后开始获取评论
        while not self.queue.empty():
            try:
                self.reviewid = self.queue.get(timeout=5)
            except StopIteration:
                break
            proxies = self.proxiespool[random.randint(0, len(self.proxiespool) - 1)]
            reviewurl = 'https://movie.douban.com/j/review/{0}/full'.format(self.reviewid)
            reviewhtml = requests.get(reviewurl, proxies=proxies)
            js = json.loads(reviewhtml.text)  # js = data.content.decode()
            html = js['body']
            soup = BeautifulSoup(html, 'html.parser').find(class_="review-content clearfix")
            title = soup["data-author"]
            reviewlist = soup.find_all('p')
            review = ''
            for item in reviewlist:
                currentreview = item.string
                if currentreview is not None:
                    review += currentreview
            self.databuffer.put(title + '\n' + review)
            print(self.name, '爬取完成影评', self.reviewid, '。程序已运行', str(time.time() - start))


# 监控数据缓冲区线程
def writefile():
    with open('影评.txt', 'w+', encoding='utf8') as f:
        while True:
            try:
                review = databuffer.get(timeout=5)
                f.write(review + '\n\n')
            except StopIteration:
                print('评论队列已空')
                break
    print('评论写入完毕')


# 启动爬虫
if __name__ == '__main__':
    fun = input('请输入 1）更新IP池 2）开启爬虫')
    if fun == '1':
        refreship()
    else:
        douban_spider()
